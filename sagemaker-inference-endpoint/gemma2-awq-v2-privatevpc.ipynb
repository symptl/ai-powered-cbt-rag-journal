{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d7d28-e838-4cb8-9425-0f34e034ed17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T04:25:06.884348Z",
     "iopub.status.busy": "2025-12-20T04:25:06.883986Z",
     "iopub.status.idle": "2025-12-20T04:25:11.028152Z",
     "shell.execute_reply": "2025-12-20T04:25:11.027043Z",
     "shell.execute_reply.started": "2025-12-20T04:25:06.884314Z"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.model import Model\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753df21-ad30-4a0e-b52f-68aefa71e540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T04:25:11.029897Z",
     "iopub.status.busy": "2025-12-20T04:25:11.029543Z",
     "iopub.status.idle": "2025-12-20T04:25:11.036989Z",
     "shell.execute_reply": "2025-12-20T04:25:11.036209Z",
     "shell.execute_reply.started": "2025-12-20T04:25:11.029820Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION (Replace these placeholders) ---\n",
    "AWS_REGION = \"INSERT AWS REGION\"\n",
    "\n",
    "# Set IAM Role to be Sagemaker Execution Role\n",
    "IAM_ROLE_ARN = \"INSERT IAM ID FOR SAGEMAKER EXECUTION ROLE\"\n",
    "\n",
    "# Model and Endpoint Details (point to model folder cloned from HF)\n",
    "S3_MODEL_PATH = \"INSERT PATH TO MODEL IN S3\" \n",
    "\n",
    "# Networking for private VPC access and security\n",
    "VPC_CONFIG = {\n",
    "    'Subnets': ['INSERT APPROPRIATE SUBNET ID', 'ADDITIONAL SUBNET IDS IF NEEDED'], \n",
    "    'SecurityGroupIds': ['INSERT SECUTIY GROUP ID'] \n",
    "}\n",
    "\n",
    "\n",
    "INSTANCE_TYPE = \"ml.g6.4xlarge\" # The GPU instance type\n",
    "ENDPOINT_NAME = f\"gemma2-9b-vllm-lmcache-prod-{datetime.now().strftime('%Y%m%d%H%M')}\"\n",
    "LMI_VERSION = \"0.30.0\" # A recent, stable LMI version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63040746-9c92-4583-828b-750db1469be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T04:25:11.037905Z",
     "iopub.status.busy": "2025-12-20T04:25:11.037604Z",
     "iopub.status.idle": "2025-12-20T04:25:11.066564Z",
     "shell.execute_reply": "2025-12-20T04:25:11.065917Z",
     "shell.execute_reply.started": "2025-12-20T04:25:11.037883Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. RETRIEVE THE LMI CONTAINER IMAGE URI ---\n",
    "# Framework \"djl-lmi\" is the identifier for the container that includes vLLM.\n",
    "lmi_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-lmi\",\n",
    "    region=AWS_REGION,\n",
    "    version=LMI_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b716c-73c7-4110-92a8-48cb09377159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T04:25:11.068428Z",
     "iopub.status.busy": "2025-12-20T04:25:11.068043Z",
     "iopub.status.idle": "2025-12-20T04:25:11.073877Z",
     "shell.execute_reply": "2025-12-20T04:25:11.073000Z",
     "shell.execute_reply.started": "2025-12-20T04:25:11.068397Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. DEFINE VLLM AND LMCACHE SETTINGS (The \"env\" Dictionary) ---\n",
    "# This is the instruction set for the container at startup.\n",
    "\n",
    "environment_vars = {\n",
    "    # 1. MODEL LOADING AND OPTIMIZATION\n",
    "    \"HF_MODEL_ID\": S3_MODEL_PATH,\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",      # CRITICAL: Activates the vLLM scheduler (Continuous Batching)\n",
    "    \"OPTION_QUANTIZE\": \"awq_marlin\",            # Loads the 4-bit AWQ hardware kernels\n",
    "    \"OPTION_DTYPE\": \"fp16\",          # Data type for model processing\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"4096\",      # Max context length (Gemma 2 default)\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\", # Use 1 GPU (Since ml.g6.4xlarge has 1 GPU) (change to match gpu count in instance type)\n",
    "\n",
    "    # Basic config (from above)\n",
    "    \"OPTION_ENGINE\": \"Python\",\n",
    "    \n",
    "    # Enable prefix caching (requires vLLM backend)\n",
    "    \"OPTION_ENABLE_PREFIX_CACHING\": \"true\",\n",
    "    \"OPTION_BLOCK_SIZE\": \"32\",  # Block size for KV cache\n",
    "    \"OPTION_MAX_NUM_SEQS\": \"256\",  # Max concurrent sequences\n",
    "    \n",
    "    # Memory management\n",
    "    \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.9\",  # Use 90% of GPU memory\n",
    "    \"OPTION_ENFORCE_EAGER\": \"false\",  # Allow CUDA graphs\n",
    "    \n",
    "    # Performance tuning\n",
    "    \"OPTION_USE_V2_BLOCK_MANAGER\": \"true\",  # Use newer block manager\n",
    "    \"OPTION_SWAP_SPACE\": \"48\"  # GB of CPU swap space\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5b8e5-91cc-4a33-8a8a-5cfac1cb1cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T04:25:11.075511Z",
     "iopub.status.busy": "2025-12-20T04:25:11.074925Z",
     "iopub.status.idle": "2025-12-20T04:25:11.081238Z",
     "shell.execute_reply": "2025-12-20T04:25:11.080631Z",
     "shell.execute_reply.started": "2025-12-20T04:25:11.075480Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. CREATE AND DEPLOY THE SAGEMAKER ENDPOINT ---\n",
    "\n",
    "print(f\"LMI Container URI: {lmi_image_uri}\")\n",
    "print(f\"Deploying model to Endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "# Create the Model Blueprint\n",
    "lmi_model = Model(\n",
    "    image_uri=lmi_image_uri,\n",
    "    role=IAM_ROLE_ARN,\n",
    "    env=environment_vars,\n",
    "    name=ENDPOINT_NAME,\n",
    "    vpc_config=VPC_CONFIG,                 # Attaches the endpoint to your private network\n",
    "    enable_network_isolation=False,         # Opens up access for DNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18aca6-105a-4656-b2c0-03e8fcbd0ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T04:25:11.082397Z",
     "iopub.status.busy": "2025-12-20T04:25:11.082037Z",
     "iopub.status.idle": "2025-12-20T04:30:43.864732Z",
     "shell.execute_reply": "2025-12-20T04:30:43.864221Z",
     "shell.execute_reply.started": "2025-12-20T04:25:11.082303Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- DEPLOY THE MODEL ---\n",
    "# This step actually creates the endpoint on AWS. It will take ~5-10 minutes.\n",
    "predictor = lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    wait=True,  # IMPORTANT: Script must pause here until deployment finishes\n",
    "    container_startup_health_check_timeout=900\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ… Deployment successful! Endpoint Name: {ENDPOINT_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
